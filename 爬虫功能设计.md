##   一、网站爬虫设计

1️⃣2️⃣3️⃣4️⃣5️⃣6️⃣7️⃣8️⃣9️⃣➡☑◼◽

### 1、请求获取页面Document

##### 1️⃣ 获取方式

- **Jsoup：**适合静态网页或页面内容以 HTML 格式直接呈现的情况

- **HttpClient：** 适用于复杂的 HTTP 请求场景，支持自定义请求头、Cookies、身份验证等。适合有特殊要求的页面抓取。

- **Selenium：**适用于动态加载内容的页面，特别是需要 JavaScript 渲染的页面。适合处理交互性内容。


##### 2️⃣方式选择

- 对于静态页面，推荐使用 **HttpClient** 配合 **Jsoup** 进行请求和解析。
- 对于动态网页，推荐使用 **Selenium** 来获取完整页面内容。

#### 2、请求管理（公共方法）

##### 1️⃣ 配置请求头（动态配置）

- 配置请求头池、代理 IP 池，支持动态更换

```
Host：目标主机的域名和端口（HTTP/1.1中是必需的）。
User-Agent：客户端软件的名称和版本（浏览器、爬虫等），用于标识请求来源。
Accept：指定客户端可接受的内容类型，如 text/html、application/json 等。
Accept-Language：指定客户端可接受的语言，例如 en-US、zh-CN 等。
Accept-Encoding：指定可接受的内容编码（如 gzip、deflate），便于服务器返回压缩响应。
Connection：控制连接（如 keep-alive 或 close）。
Referer：来源页面的URL，用于指示用户从哪个页面链接过来。
Authorization：携带身份验证信息（如Bearer Token或Basic Auth信息）。
Cookie：携带之前从服务器接收到的Cookie信息，以维持会话。
Content-Type：在POST请求中，指示随请求发送的数据类型，例如 application/x-www-form-urlencoded 或 application/json。
```

- 配置请求头池、代理 IP池可·提供·更换等

##### 2️⃣ 配置请求频率

- 为了避免被封禁，设置请求间隔，且每次请求之间加入随机延迟。、
- 根据返回的 HTTP 状态码（如 403、404、429 等）进行适当处理。例如，针对 429 错误（请求过多）可以降低请求频率或增加重试间隔。

#### 3、**数据解析**

##### 1️⃣ 解析方式

- **Jsoup**：适用于静态网页和简单的结构化数据提取。
- **XPath**：适合处理复杂的 HTML 或 XML 结构，精确定位目标数据。

#### 4、**调度管理**：

##### 1️⃣调度方式

- 支持手动调度。
- 可以为每个网站配置定时任务，或将多个网站分组并为分组配置统一的抓取时间。
- 定时任务支持动态调整。

#### 2️⃣支持多线程

- 多线程抓取支持，提高抓取效率。
- 线程池管理可以避免因过多线程导致系统资源浪费

#### 5、**数据存储、**数据去重 ：

##### 1️⃣ 数据存储

- 支持将抓取的数据存储到数据库（如 MySQL、MongoDB）或导出为 Excel 格式。

- 采用批量写入和异步存储的方式，解耦抓取与存储过程，提升效率。


##### 2️⃣数据去重

- 基于URL去重

  > 1、在数据库中为 URL 添加唯一索引，避免重复抓取。
  > 2、通过 URL 哈希值（如使用 MD5、SHA256）来去重，存储哈希值并添加索引。这样不仅节省存储空间，还能提高查询效率。
  >
  > ```
  > 对url进行哈希处理、推荐使用哈希算法 SHA-256
  >import java.security.MessageDigest;
  > import java.security.NoSuchAlgorithmException;
  > public class URLHashing {
  >     // 对 URL 进行哈希处理
  >     public static String hashURL(String url) {
  >         try {
  >             // 创建 SHA-256 哈希实例
  >             MessageDigest digest = MessageDigest.getInstance("SHA-256");
  >             // 计算哈希值
  >             byte[] hashBytes = digest.digest(url.getBytes());
  >             // 将字节数组转换为十六进制字符串
  >             StringBuilder hexString = new StringBuilder();
  >             for (byte b : hashBytes) {
  >                 String hex = Integer.toHexString(0xff & b);
  >                 if (hex.length() == 1) hexString.append('0');
  >                 hexString.append(hex);
  >             }
  >             return hexString.toString(); // 返回哈希值
  >         } catch (NoSuchAlgorithmException e) {
  >             throw new RuntimeException("Hashing algorithm not found");
  >         }
  >     }
  >     public static void main(String[] args) {
  >         String url = "https://example.com/some/path";
  >         String hashedURL = hashURL(url);
  >         System.out.println("Original URL: " + url);
  >         System.out.println("Hashed URL: " + hashedURL);
  >     }
  > }
  > ```
  
- 基于内容去重

  > 对抓取的页面内容也进行哈希处理，避免抓取重复的内容。

##### 3️⃣ 方便数据去重优化

- 根据需求，可以通过标记旧数据，仅在新数据中进行比对，提升去重效率。
- 如果抓取的数据有时间戳，可以通过时间范围来限定抓取的数据范围，避免过期数据重复抓取。

#### 6、**错误处理机制**：

##### 1️⃣根据错误类型处理

- 针对不同错误类型（如网络错误、超时、解析失败等）进行分类处理。
- 对临时性错误设置重试机制，限制最大重试次数和重试间隔，防止因错误无限重试影响爬虫进程。

##### 2️⃣ 抓取监控

- 引入监控系统（如 Prometheus、Grafana），实时监测爬虫的运行状态、抓取进度和错误情况，及时告警。

##### 3️⃣异常预警

#### 1、**异常日志记录**：

##### 1️⃣日志分类

- 请求异常（网络问题）、解析异常（列表解析异常、文章解析异常）、数据处理异常
- 记录信息：核心信息记录，详细信息记录

#### 7、**反爬技术应对**：

- **验证码识别：** 使用 OCR 技术或第三方服务来识别验证码。
- **模拟浏览器行为：** 设置合适的 User-Agent、Referer、Cookie 等头信息，模拟真实用户访问，避免被反爬虫系统拦截。
- **Cookies 管理：** 持续维护和更新 Cookies，以保持会话活跃，防止被服务器识别为非正常访问。

#### 8、**配置管理**：

##### 1️⃣ 可配置参数

- 定时任务时间，超时时间，执行间隔，邮箱，用户代理（User-Agent），请求头

9、**数据分析与可视化**：

- 对抓取的数据进行分析，生成统计报表、趋势图等，可视化结果，帮助用户更直观地理解数据。

- 可以使用第三方工具（如 Tableau、PowerBI）进行更复杂的分析和可视化。

10、**数据完整性检查：**

- 定期检查抓取的数据完整性，确保没有漏抓关键字段或数据。

- 对抓取的核心字段（如 ID、URL、标题等）进行校验，确保数据的一致性。

  

## 二、公众号抓取设计





## 三、动态配置解析

